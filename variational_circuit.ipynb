{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5be068-0d62-4584-b647-0529ae996157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in /opt/conda/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.9.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.23.5)\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (5.3.1)\n",
      "Requirement already satisfied: pennylane-lightning>=0.33 in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.33.1)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.5.0)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.6.7)\n",
      "Requirement already satisfied: rustworkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.1)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autograd in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.28.2)\n",
      "Requirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.10/site-packages (from autograd->pennylane) (0.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (1.26.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527589eb-81a2-43d4-97a6-b8a4774a45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "### DO NOT CHANGE ANYTHING BELOW THIS LINE\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "WIRES = 2\n",
    "LAYERS = 5\n",
    "NUM_PARAMETERS = LAYERS * WIRES * 3\n",
    "\n",
    "def variational_circuit(params,hamiltonian):\n",
    "    \"\"\"\n",
    "    This is a template variational quantum circuit containing a fixed layout of gates with variable\n",
    "    parameters. To be used as a QNode, it must either be wrapped with the @qml.qnode decorator or\n",
    "    converted using the qml.QNode function.\n",
    "\n",
    "    The output of this circuit is the expectation value of a Hamiltonian, somehow encoded in\n",
    "    the hamiltonian argument\n",
    "\n",
    "    Args:\n",
    "        - params (np.ndarray): An array of optimizable parameters of shape (30,)\n",
    "        - hamiltonian (np.ndarray): An array of real parameters encoding the Hamiltonian\n",
    "        whose expectation value is returned.\n",
    "    \n",
    "    Returns:\n",
    "        (float): The expectation value of the Hamiltonian\n",
    "    \"\"\"\n",
    "    parameters = params.reshape((LAYERS, WIRES, 3))\n",
    "    qml.templates.StronglyEntanglingLayers(parameters, wires=range(WIRES))\n",
    "    return qml.expval(qml.Hermitian(hamiltonian, wires = [0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee2f652-3f5e-44c8-9bba-104845deaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_circuit(hamiltonian, optimizer_name, stepsize):\n",
    "    \"\"\"Minimize the variational circuit and return its minimum value.\n",
    "    You should create a device and convert the variational_circuit function \n",
    "    into an executable QNode. \n",
    "    Next, you should minimize the variational circuit using gradient-based \n",
    "    optimization to update the input params. \n",
    "    Return the optimized value of the QNode as a single floating-point number.\n",
    "\n",
    "    Args:\n",
    "        - params (np.ndarray): Input parameters to be optimized, of dimension 30\n",
    "        - hamiltonian (np.ndarray): An array of real parameters encoding the Hamiltonian\n",
    "        whose expectation value you should minimize.\n",
    "    Returns:\n",
    "        float: the value of the optimized QNode\n",
    "    \"\"\"\n",
    "        \n",
    "    hamiltonian = np.array(hamiltonian, requires_grad = False)\n",
    "\n",
    "    hamiltonian = np.array(hamiltonian,float).reshape((2 ** WIRES), (2 ** WIRES))\n",
    "\n",
    "    ### WRITE YOUR CODE BELOW THIS LINE\n",
    "    \n",
    "    ### Solution Template\n",
    "    dev = qml.device('default.qubit', wires=WIRES) # Initialize the device.\n",
    "    circuit = qml.QNode(variational_circuit, dev) # Instantiate the QNode from variational_circuit.\n",
    "\n",
    "    def objective(params):\n",
    "        return circuit(params, hamiltonian)\n",
    "\n",
    "    # Minimize the circuit\n",
    "    if optimizer_name == 'GRAD':\n",
    "        opt = qml.GradientDescentOptimizer(stepsize=stepsize)\n",
    "    elif optimizer_name == 'Momentum':\n",
    "        opt = qml.MomentumOptimizer(stepsize=stepsize)\n",
    "    elif optimizer_name == 'NesterovMomentum':\n",
    "        opt = qml.NesterovMomentumOptimizer(stepsize=stepsize)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        opt = qml.AdagradOptimizer(stepsize=stepsize)\n",
    "    elif optimizer_name == 'RMSProp':\n",
    "        opt = qml.RMSPropOptimizer(stepsize=stepsize)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        opt = qml.AdamOptimizer(stepsize=stepsize)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {optimizer_name} not recognized.\")\n",
    "        \n",
    "    params = np.random.rand(NUM_PARAMETERS)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    steps = 100\n",
    "\n",
    "    for _ in range(steps):\n",
    "        params = opt.step(objective, params)\n",
    "\n",
    "    # Otra forma de minimizar usando Scipy\n",
    "    # from scipy.optimize import minimize\n",
    "    # result = minimize(objective, params, method='BFGS')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return round(float(circuit(params, hamiltonian)), 8), elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f2ef30-aa51-4b0c-a1cc-b93c8ef6a6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: GRAD, Result: 0.62925957, Elapsed Time: 6.559036731719971 seconds\n",
      "Optimizer: Momentum, Result: 0.61747441, Elapsed Time: 6.019376993179321 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745345, Elapsed Time: 6.259066104888916 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 6.079759359359741 seconds\n",
      "Optimizer: RMSProp, Result: 0.62529663, Elapsed Time: 6.34599494934082 seconds\n",
      "Optimizer: Adam, Result: 0.61745383, Elapsed Time: 6.29037070274353 seconds\n",
      "Runtime for stepsize 0.1: 6.258934140205383 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61780125, Elapsed Time: 6.080897331237793 seconds\n",
      "Optimizer: Momentum, Result: 0.61745511, Elapsed Time: 5.965817213058472 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.6174536, Elapsed Time: 5.947551727294922 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 6.021591424942017 seconds\n",
      "Optimizer: RMSProp, Result: 0.62813938, Elapsed Time: 5.883145332336426 seconds\n",
      "Optimizer: Adam, Result: 0.61745504, Elapsed Time: 6.04702091217041 seconds\n",
      "Runtime for stepsize 0.2: 5.99100399017334 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.6174559, Elapsed Time: 5.92506742477417 seconds\n",
      "Optimizer: Momentum, Result: 0.61745729, Elapsed Time: 5.930109024047852 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745342, Elapsed Time: 5.778196334838867 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 5.945321798324585 seconds\n",
      "Optimizer: RMSProp, Result: 0.63360912, Elapsed Time: 6.173421621322632 seconds\n",
      "Optimizer: Adam, Result: 0.61745446, Elapsed Time: 6.247201204299927 seconds\n",
      "Runtime for stepsize 0.30000000000000004: 5.999886234601338 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745343, Elapsed Time: 6.166074752807617 seconds\n",
      "Optimizer: Momentum, Result: 0.61745462, Elapsed Time: 5.8913960456848145 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 6.09314489364624 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 6.054579734802246 seconds\n",
      "Optimizer: RMSProp, Result: 0.76910608, Elapsed Time: 6.167228937149048 seconds\n",
      "Optimizer: Adam, Result: 0.61745621, Elapsed Time: 6.0336902141571045 seconds\n",
      "Runtime for stepsize 0.4: 6.067685763041179 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745344, Elapsed Time: 6.141456127166748 seconds\n",
      "Optimizer: Momentum, Result: 0.61745394, Elapsed Time: 6.148805856704712 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 6.125795602798462 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 6.041633129119873 seconds\n",
      "Optimizer: RMSProp, Result: 0.76083741, Elapsed Time: 5.875012397766113 seconds\n",
      "Optimizer: Adam, Result: 0.61747592, Elapsed Time: 6.034526109695435 seconds\n",
      "Runtime for stepsize 0.5: 6.06120487054189 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745341, Elapsed Time: 6.039580345153809 seconds\n",
      "Optimizer: Momentum, Result: 0.61745548, Elapsed Time: 5.98192572593689 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 5.853196620941162 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 6.122859477996826 seconds\n",
      "Optimizer: RMSProp, Result: 0.73542101, Elapsed Time: 6.012231826782227 seconds\n",
      "Optimizer: Adam, Result: 0.61745743, Elapsed Time: 6.0173704624176025 seconds\n",
      "Runtime for stepsize 0.6: 6.004527409871419 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745341, Elapsed Time: 6.048578262329102 seconds\n",
      "Optimizer: Momentum, Result: 0.61745481, Elapsed Time: 5.789575576782227 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 5.90355110168457 seconds\n",
      "Optimizer: Adagrad, Result: 0.61746921, Elapsed Time: 5.956407070159912 seconds\n",
      "Optimizer: RMSProp, Result: 0.7910664, Elapsed Time: 5.934078931808472 seconds\n",
      "Optimizer: Adam, Result: 0.61889962, Elapsed Time: 5.769175291061401 seconds\n",
      "Runtime for stepsize 0.7000000000000001: 5.900227705637614 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745341, Elapsed Time: 5.903717994689941 seconds\n",
      "Optimizer: Momentum, Result: 0.61745452, Elapsed Time: 5.911423206329346 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 5.941415309906006 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 5.936236381530762 seconds\n",
      "Optimizer: RMSProp, Result: 0.85231351, Elapsed Time: 5.76054835319519 seconds\n",
      "Optimizer: Adam, Result: 0.61745702, Elapsed Time: 5.945312738418579 seconds\n",
      "Runtime for stepsize 0.8: 5.899775664011638 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745341, Elapsed Time: 5.933689832687378 seconds\n",
      "Optimizer: Momentum, Result: 0.61745643, Elapsed Time: 5.934891223907471 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 5.7927467823028564 seconds\n",
      "Optimizer: Adagrad, Result: 0.6174585, Elapsed Time: 5.912339687347412 seconds\n",
      "Optimizer: RMSProp, Result: 0.71463526, Elapsed Time: 5.945239305496216 seconds\n",
      "Optimizer: Adam, Result: 0.62939361, Elapsed Time: 5.9320573806762695 seconds\n",
      "Runtime for stepsize 0.9: 5.908494035402934 seconds\n",
      "\n",
      "Optimizer: GRAD, Result: 0.61745341, Elapsed Time: 5.920647859573364 seconds\n",
      "Optimizer: Momentum, Result: 0.61745477, Elapsed Time: 5.77293848991394 seconds\n",
      "Optimizer: NesterovMomentum, Result: 0.61745341, Elapsed Time: 5.93876576423645 seconds\n",
      "Optimizer: Adagrad, Result: 0.61745341, Elapsed Time: 5.982795000076294 seconds\n",
      "Optimizer: RMSProp, Result: 0.85069929, Elapsed Time: 5.9521074295043945 seconds\n",
      "Optimizer: Adam, Result: 0.62089928, Elapsed Time: 5.879557132720947 seconds\n",
      "Runtime for stepsize 1.0: 5.9078019460042315 seconds\n",
      "\n",
      "Best Optimizer: NesterovMomentum, Best Result: 0.61745341, Best Stepsize: 0.9, Best Runtime: 5.7927467823028564 seconds\n"
     ]
    }
   ],
   "source": [
    "test_input = [0.863327072347624,0.0167108057202516,0.07991447085492759,0.0854049026262154, 0.0167108057202516,0.8237963773906136,-0.07695947154193797,0.03131548733285282, 0.07991447085492759,-0.07695947154193795,0.8355417021014687,-0.11345916130631205, 0.08540490262621539,0.03131548733285283,-0.11345916130631205,0.758156886827099]\n",
    "optimizer_names = ['GRAD', 'Momentum', 'NesterovMomentum', 'Adagrad', 'RMSProp', 'Adam']\n",
    "stepsizes = np.linspace(0.1, 1, 10)\n",
    "\n",
    "target_result = 0.61745341\n",
    "\n",
    "best_result = float('inf')\n",
    "best_stepsize = None\n",
    "best_optimizer = None\n",
    "best_runtime = float('inf')\n",
    "\n",
    "for stepsize in stepsizes:\n",
    "    runtime_per_stepsize = []\n",
    "\n",
    "    for optimizer_name in optimizer_names:\n",
    "        result, elapsed_time = optimize_circuit(test_input, optimizer_name, stepsize)\n",
    "        print(f\"Optimizer: {optimizer_name}, Result: {result}, Elapsed Time: {elapsed_time} seconds\")\n",
    "\n",
    "        runtime_per_stepsize.append(elapsed_time)\n",
    "\n",
    "        # Si el resultado es igual al valor objetivo y el tiempo de ejecuci√≥n es menor, actualiza los mejores valores\n",
    "        if result == target_result and elapsed_time < best_runtime:\n",
    "            best_result = result\n",
    "            best_stepsize = stepsize\n",
    "            best_optimizer = optimizer_name\n",
    "            best_runtime = elapsed_time\n",
    "\n",
    "    print(f\"Runtime for stepsize {stepsize}: {np.mean(runtime_per_stepsize)} seconds\\n\")\n",
    "\n",
    "print(f\"Best Optimizer: {best_optimizer}, Best Result: {best_result}, Best Stepsize: {best_stepsize}, Best Runtime: {best_runtime} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
